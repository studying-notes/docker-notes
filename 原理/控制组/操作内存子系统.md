# 操作内存子系统（memory subsystem）

> 通过 memory subsystem 限制 cgroup 中内存的占用。

## 可限制的方面

- 物理内存总量。
- 虚拟内存总量，即物理内存加上交换空间的总量（CONFIG_MEMCG_SWAP）。
- 内核可用内存总量及其它内核资源（CONFIG_MEMCG_KMEM），限制内核内存就是限制当前控制组所能使用的内核资源，比如进程的内核栈空间，socket 所占用的内存空间等。通过限制内核内存，当内存吃紧时，可以阻止当前控制组继续创建进程以及向内核申请分配更多的内核资源。

## 内核相关配置

由于 `memory subsystem` 比较耗资源，所以内核专门添加了一个参数 `cgroup_disable=memory` 来禁用整个 `memory subsystem`，这个参数可以通过 GRUB 在启动系统的时候传给内核，加了这个参数后内核将不再进行 `memory subsystem` 相关的计算工作，在系统中也不能挂载 `memory subsystem`。另外，`CONFIG_MEMCG_SWAP` 和 `CONFIG_MEMCG_KMEM` 都是扩展功能，不同发行版 Linux 内核不一定都支持。

**确认当前是否内核支持扩展**

```bash
$ cat /boot/config-`uname -r` | grep CONFIG_MEMCG
CONFIG_MEMCG=y
CONFIG_MEMCG_SWAP=y
# CONFIG_MEMCG_SWAP_ENABLED is not set
```

这里 Ubuntu 18.04.3 确实没有 `CONFIG_MEMCG_KMEM` 这个选项配置，但实际上应该还是支持的。

## 限制内存占用示例

1. 当前已绑定的目录

```bash
$ mount | grep memory
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)
```

可以看到，`/sys/fs/cgroup/memory` 目录已经挂载在了 `memory subsystem` 的 `hierarchy` 上，正如前面所说，系统开机时默认会进行挂载。

2. 在不做限制的情况下，启动一个占用内存的 stress 进程

```bash
# apt install stress
$ stress --vm-bytes 100m --vm-keep -m 1

$ top -n 1
PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND       
31846 root      20   0  110644 102700    212 R 93.3 10.2   2:32.30 stress      

# 实验机器内存1GB，占用10.2%
# 终止该进程
```

3. 创建一个子 cgroup

```bash
$ cd /sys/fs/cgroup/memory
$ mkdir limit-memory && cd limit-memory

$ ls
cgroup.clone_children               memory.memsw.failcnt
cgroup.event_control                memory.memsw.limit_in_bytes
cgroup.procs                        memory.memsw.max_usage_in_bytes
memory.failcnt                      memory.memsw.usage_in_bytes
memory.force_empty                  memory.move_charge_at_immigrate
memory.kmem.failcnt                 memory.numa_stat
memory.kmem.limit_in_bytes          memory.oom_control
memory.kmem.max_usage_in_bytes      memory.pressure_level
memory.kmem.slabinfo                memory.soft_limit_in_bytes
memory.kmem.tcp.failcnt             memory.stat
memory.kmem.tcp.limit_in_bytes      memory.swappiness
memory.kmem.tcp.max_usage_in_bytes  memory.usage_in_bytes
memory.kmem.tcp.usage_in_bytes      memory.use_hierarchy
memory.kmem.usage_in_bytes          notify_on_release
memory.limit_in_bytes               tasks
memory.max_usage_in_bytes
```

**这些文件的作用**

- **cgroup.event_control** 用于 `eventfd` 的接口。
- **memory.usage_in_bytes** 显示当前已用的内存。
- **memory.limit_in_bytes** 设置/显示当前限制的内存额度，单位是字节，也可以使用 k/K、m/M 或者 g/G 表示要设置数值的单位。
- **memory.memsw.limit_in_bytes** 设定内存加上交换分区的可用总量，即虚拟内存总量。通过设置这个值，可以防止进程把系统交换分区用光。
- **memory.failcnt** 显示内存使用量达到限制值的次数。
- **memory.max_usage_in_bytes** 历史内存最大使用量。
- **memory.soft_limit_in_bytes** 设置/显示当前限制的内存软额度。
- **memory.stat** 显示当前 cgroup 的内存使用情况。
- **memory.use_hierarchy** 设置/显示是否将子 cgroup 的内存使用情况统计到当前 cgroup 里面。
- **memory.force_empty** 触发系统立即尽可能的回收当前 cgroup 中可以回收的内存。
- **memory.pressure_level** 设置内存压力的通知事件，配合 `cgroup.event_control` 一起使用。
- **memory.swappiness** 设置和显示当前的 `swappiness`。
- **memory.oom_control** 设置/显示 `oomcontrols` 相关的配置，如果设置为0，那么在内存使用量超过上限时，系统不会“杀死”进程，而是阻塞进程直到有内存被释放可供使用时；另一方面，系统会向用户态发送事件通知，用户态的监控程序可以根据该事件来做相应的处理，例如提高内存上限等。
- **memory.numa_stat** 显示 `numa` 相关的内存信息。
- **memory.move_charge_at_immigrate** 设置当进程移动到其他 cgroup 中时，它所占用的内存是否也随着移动过去。当一个进程从一个 cgroup 移动到另一个 cgroup 时，默认情况下，该进程已经占用的内存还是统计在原来的 cgroup 里面，不会占用新 cgroup 的配额，但新分配的内存会统计到新的 cgroup 中。

```
enable： echo 1 > memory.move_charge_at_immigrate
disable：echo 0 > memory.move_charge_at_immigrate
```

> **⚠️**：就算设置为1，如果以线程为单位进行迁移，必须是进程的第一个线程，否则这个进程占用的内存也不能被迁移过去，如果以进程为单位进行迁移，就没有这个问题。迁移内存占用数据是比较耗时的操作。

4. 添加进程

```bash
$ echo $$
17656

$ echo $$ >> cgroup.procs

# 启动一个占用100MB内存的 stress 进程
$ stress --vm-bytes 100m --vm-keep -m 1

# 别关窗口
```

5. 另开一个窗口，设置限额

```bash
$ cd /sys/fs/cgroup/memory/limit-memory
$ cat cgroup.procs
17656
17949
17950

# 设置200M限额
$ echo 200M > memory.limit_in_bytes
# 因为页对齐, 生效的数量不一定完全等于设置的数量

# 撤销限额
$ echo -1 > memory.limit_in_bytes
```

6. 尝试让限额低于当前已用

```bash
$ echo 90M > memory.limit_in_bytes
bash: echo: write error: Device or resource busy
# 无法设置
```

7. 尝试让程序所需内存超过限额

```bash
$ stress --vm-bytes 200m --vm-keep -m 1
stress: info: [31864] dispatching hogs: 0 cpu, 0 io, 1 vm, 0 hdd
stress: FAIL: [31864] (415) <-- worker 31865 got signal 9
stress: WARN: [31864] (417) now reaping child worker processes
stress: FAIL: [31864] (451) failed run completed in 0s
# 无法创建

# 结束内存占用程序
```

## 触发 Out Of Memory

当物理内存达到上限后，系统的默认行为是 `kill` 掉 cgroup 中继续申请内存的进程，或者自行配置 `memory.oom_control` 修改行为。这个文件包含了一个控制是否为当前 cgroup 启动 OOM-killer 的标识：

- 0：表示将启动 OOM-killer，当内核无法给进程分配足够的内存时，将会直接 `kill` 掉该进程。
- 1：表示不启动 OOM-killer，当内核无法给进程分配足够的内存时，将会暂停该进程直到有空余的内存之后再继续运行，这种情况下，`memory.oom_control` 还将包含一个只读的 `under_oom` 字段，用来表示当前是否已经进入 OOM-killer 状态，也即是否有进程被暂停了。

> root cgroup 的 OOM-killer 是无法被禁用的。

1. 创建一个程序，向系统申请内存，每秒消耗1M的内存

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>

#define MB (1024 * 1024)

int main(int argc, char *argv[])
{
    char *p;
    int i = 0;
    while (1)
    {
        p = (char *)malloc(MB);
        memset(p, 0, MB);
        printf("%dM memory allocated.\n", ++i);
        sleep(1);
    }

    return 0;
}
```

2. 编译、测试限额

```bash
$ gcc ~/memory.c -o ~/memory
$ echo 5M > memory.limit_in_bytes
$ echo $$ >> cgroup.procs

$ cat memory.oom_control
oom_kill_disable 0
under_oom 0
oom_kill 1
# 默认情况下，memory.oom_control 的值为0，即默认直接 kill 掉该进程

# 禁用交换空间
$ echo 0 > memory.swappiness
$ cat memory.swappiness
0

$ ~/memory
1M memory allocated.
2M memory allocated.
3M memory allocated.
4M memory allocated.
Killed
# 当分配第5M内存时，由于总内存量超过了5M，所以进程被 kill 了

# 设置 oom_control 为1，这样内存达到限额的时候会暂停
$ echo 1 >> memory.oom_control
$ ~/memory
……
# 被暂停了
```

3. 另开一个窗口，提高限额

```bash
$ cd /sys/fs/cgroup/memory/limit-memory
$ cat memory.oom_control
oom_kill_disable 1
under_oom 1
oom_kill 2

# 修改限额为10M
$ echo 10M > memory.limit_in_bytes

# 切换到原来的窗口，发现进程继续执行了几次，然后暂停了
```

### 章节导航

- 上一节：[操作 CPU 子系统](操作%20CPU%20子系统.md)
- 下一节：[自制一个限制内存的容器](自制一个限制内存的容器.md)
